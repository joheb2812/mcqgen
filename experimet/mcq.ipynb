{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a624841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json\n",
    "import pandas as pd \n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "468e12c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "358789a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "168ede07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIzaSyA02zMDB8hwaa0kEQXXyHYd4e7aL8MolQM\n"
     ]
    }
   ],
   "source": [
    "print(KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f0636b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "client = ChatGoogleGenerativeAI(api_key=KEY, model=\"gemini-1.5-flash\", temperature=0.1) \n",
    "prompt = \"Can you tell me total numbers of country in asia? Can you give me top 10 country name?\"\n",
    "response = client.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "697856af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There's no single universally agreed-upon number of countries in Asia.  The count depends on how you define \"country\" (e.g., including or excluding partially recognized states, territories, and dependencies) and the geographical boundaries of Asia itself.  Estimates typically range from 48 to 50.\n",
      "\n",
      "Therefore, I cannot give a definitive \"total number\".\n",
      "\n",
      "Providing a \"top 10\" list is also problematic because \"top\" implies a ranking based on some criteria (population, GDP, area, etc.).  Without knowing the ranking criteria, I cannot provide a meaningful top 10 list.  Different criteria will yield different lists.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61aeaa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c03909c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESPONSE_JSON = {\n",
    "    \"1\": {\n",
    "        \"mcq\": \"multiple choice question\",\n",
    "        \"options\": {\n",
    "            \"a\": \"choice here\",\n",
    "            \"b\": \"choice here\",\n",
    "            \"c\": \"choice here\",\n",
    "            \"d\": \"choice here\",\n",
    "        },\n",
    "        \"correct\": \"correct answer\",\n",
    "    },\n",
    "    \"2\": {\n",
    "        \"mcq\": \"multiple choice question\",\n",
    "        \"options\": {\n",
    "            \"a\": \"choice here\",\n",
    "            \"b\": \"choice here\",\n",
    "            \"c\": \"choice here\",\n",
    "            \"d\": \"choice here\",\n",
    "        },\n",
    "        \"correct\": \"correct answer\",\n",
    "    },\n",
    "    \"3\": {\n",
    "        \"mcq\": \"multiple choice question\",\n",
    "        \"options\": {\n",
    "            \"a\": \"choice here\",\n",
    "            \"b\": \"choice here\",\n",
    "            \"c\": \"choice here\",\n",
    "            \"d\": \"choice here\",\n",
    "        },\n",
    "        \"correct\": \"correct answer\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e23a0377",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE=\"\"\"\n",
    "Text:{text}\n",
    "You are an expert MCQ maker. Given the above text, it is your job to \\\n",
    "create a quiz  of {number} multiple choice questions for {subject} students in {tone} tone. \n",
    "Make sure the questions are not repeated and check all the questions to be conforming the text as well.\n",
    "Make sure to format your response like  RESPONSE_JSON below  and use it as a guide. \\\n",
    "Ensure to make {number} MCQs\n",
    "### RESPONSE_JSON\n",
    "{response_json}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7987270d",
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_generation_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\", \"number\", \"subject\", \"tone\", \"response_json\"],\n",
    "    template=TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fefc49e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(api_key=KEY, model=\"gemini-1.5-flash\", temperature=0.1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0335fd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joheb.dani\\AppData\\Local\\Temp\\ipykernel_8860\\4109068563.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  quiz_chain = LLMChain(llm=llm, prompt=quiz_generation_prompt, output_key=\"quiz\", verbose=True)\n"
     ]
    }
   ],
   "source": [
    "quiz_chain = LLMChain(llm=llm, prompt=quiz_generation_prompt, output_key=\"quiz\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0aaa9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE2=\"\"\"\n",
    "You are an expert english grammarian and writer. Given a Multiple Choice Quiz for {subject} students.\\\n",
    "You need to evaluate the complexity of the question and give a complete analysis of the quiz. Only use at max 50 words for complexity analysis. \n",
    "if the quiz is not at per with the cognitive and analytical abilities of the students,\\\n",
    "update the quiz questions which needs to be changed and change the tone such that it perfectly fits the student abilities\n",
    "Quiz_MCQs:\n",
    "{quiz}\n",
    "\n",
    "Check from an expert English Writer of the above quiz:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fadb8e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_evalution_prompt = PromptTemplate(input_variables=[\"subject\", \"quiz\"],\n",
    "               template=TEMPLATE2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "573a60a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_chain = LLMChain(llm=llm, prompt=quiz_generation_prompt, verbose=True, output_key=\"review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ddfa6631",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_evalute_chain = SequentialChain(chains=[quiz_chain, review_chain], input_variables=[\"text\", \"number\", \"subject\", \"tone\", \"response_json\"], output_variables=[\"quiz\", \"review\"], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac06021",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\joheb.dani\\mcqgen\\experimet\\data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8716f1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joheb.dani\\mcqgen\\experimet\\mcq.ipynb\n"
     ]
    }
   ],
   "source": [
    "print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f5a262e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    TEXT = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "00128ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d37a610e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"1\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"2\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"3\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}}'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.dumps(RESPONSE_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6425ea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER = 3\n",
    "SUBJECT = \"Biology\"\n",
    "TONE = \"simple\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "af95a7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joheb.dani\\AppData\\Local\\Temp\\ipykernel_8860\\1573062546.py:2: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = generate_evalute_chain({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Text:\n",
      "You are an expert MCQ maker. Given the above text, it is your job to create a quiz  of 3 multiple choice questions for Biology students in simple tone. \n",
      "Make sure the questions are not repeated and check all the questions to be conforming the text as well.\n",
      "Make sure to format your response like  RESPONSE_JSON below  and use it as a guide. Ensure to make 3 MCQs\n",
      "### RESPONSE_JSON\n",
      "{\"1\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"2\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"3\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}}\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Text:\n",
      "You are an expert MCQ maker. Given the above text, it is your job to create a quiz  of 3 multiple choice questions for Biology students in simple tone. \n",
      "Make sure the questions are not repeated and check all the questions to be conforming the text as well.\n",
      "Make sure to format your response like  RESPONSE_JSON below  and use it as a guide. Ensure to make 3 MCQs\n",
      "### RESPONSE_JSON\n",
      "{\"1\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"2\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"3\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}}\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#How to setup Token Usage Tracking in LangChain\n",
    "response = generate_evalute_chain({\n",
    "    \"text\": TEXT,\n",
    "    \"number\": NUMBER,\n",
    "    \"subject\": SUBJECT,\n",
    "    \"tone\": TONE,\n",
    "    \"response_json\": json.dumps(RESPONSE_JSON)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "baec5932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '', 'number': 3, 'subject': 'Biology', 'tone': 'simple', 'response_json': '{\"1\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"2\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"3\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}}', 'quiz': '```json\\n{\\n  \"1\": {\\n    \"mcq\": \"Which of the following is NOT a characteristic of a good multiple choice question (MCQ)?\",\\n    \"options\": {\\n      \"a\": \"Clear and concise stem\",\\n      \"b\": \"Unambiguous options\",\\n      \"c\": \"Only one correct answer\",\\n      \"d\": \"Multiple correct answers\"\\n    },\\n    \"correct\": \"d\"\\n  },\\n  \"2\": {\\n    \"mcq\": \"What is the primary goal of creating multiple choice questions for Biology students?\",\\n    \"options\": {\\n      \"a\": \"To trick the students\",\\n      \"b\": \"To assess student understanding of biological concepts\",\\n      \"c\": \"To fill up the exam paper\",\\n      \"d\": \"To make the exam difficult\"\\n    },\\n    \"correct\": \"b\"\\n  },\\n  \"3\": {\\n    \"mcq\": \"In creating MCQs, what is crucial to ensure the questions are conforming to the given text?\",\\n    \"options\": {\\n      \"a\": \"Ignoring the text completely\",\\n      \"b\": \"Making sure the questions are unrelated to the text\",\\n      \"c\": \"Relating the questions to the instructions and context provided\",\\n      \"d\": \"Using only information not mentioned in the text\"\\n    },\\n    \"correct\": \"c\"\\n  }\\n}\\n```', 'review': '```json\\n{\\n  \"1\": {\\n    \"mcq\": \"Which of the following is NOT a characteristic of a good multiple choice question (MCQ)?\",\\n    \"options\": {\\n      \"a\": \"Clear and concise stem\",\\n      \"b\": \"Unambiguous options\",\\n      \"c\": \"Only one correct answer\",\\n      \"d\": \"Multiple correct answers\"\\n    },\\n    \"correct\": \"d\"\\n  },\\n  \"2\": {\\n    \"mcq\": \"What is the primary purpose of creating multiple choice questions for Biology students?\",\\n    \"options\": {\\n      \"a\": \"To confuse students\",\\n      \"b\": \"To assess understanding of biological concepts\",\\n      \"c\": \"To fill up exam time\",\\n      \"d\": \"To make the exam difficult\"\\n    },\\n    \"correct\": \"b\"\\n  },\\n  \"3\": {\\n    \"mcq\": \"In creating effective MCQs, what is crucial to ensure?\",\\n    \"options\": {\\n      \"a\": \"The questions are very long and complex.\",\\n      \"b\": \"The options are similar in length and style to avoid bias.\",\\n      \"c\": \"The questions are unrelated to the learning objectives.\",\\n      \"d\": \"There are no distractor options.\"\\n    },\\n    \"correct\": \"b\"\\n  }\\n}\\n```'}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "749d902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz = response.get(\"quiz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0c8a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n  \"1\": {\\n    \"mcq\": \"Which of the following is NOT a characteristic of a good multiple choice question (MCQ)?\",\\n    \"options\": {\\n      \"a\": \"Clear and concise stem\",\\n      \"b\": \"Unambiguous options\",\\n      \"c\": \"Only one correct answer\",\\n      \"d\": \"Multiple correct answers\"\\n    },\\n    \"correct\": \"d\"\\n  },\\n  \"2\": {\\n    \"mcq\": \"What is the primary goal of creating multiple choice questions for Biology students?\",\\n    \"options\": {\\n      \"a\": \"To trick the students\",\\n      \"b\": \"To assess student understanding of biological concepts\",\\n      \"c\": \"To fill up the exam paper\",\\n      \"d\": \"To make the exam difficult\"\\n    },\\n    \"correct\": \"b\"\\n  },\\n  \"3\": {\\n    \"mcq\": \"In creating MCQs, what is crucial to ensure the questions are conforming to the given text?\",\\n    \"options\": {\\n      \"a\": \"Ignoring the text completely\",\\n      \"b\": \"Making sure the questions are unrelated to the text\",\\n      \"c\": \"Relating the questions to the instructions and context provided\",\\n      \"d\": \"Using only information not mentioned in the text\"\\n    },\\n    \"correct\": \"c\"\\n  }\\n}\\n```'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87f416b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
